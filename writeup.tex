\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}

\title{Luck vs Skill in College Basketball}
\author{Seth Peacock, Jake Browning, Rohan Mawalkar}
\date{November 2024}

\begin{document}

\maketitle

\newpage

\section{Letter to the Editor}
In addition to this report, write a one-page letter to the newspaper chief editor, explaining the main results of
the report and suggesting findings that can be communicated with the basketball fans reading the newspaper.

The bottom line is that the tournament is just too uncertain to expect to be able to predict. But we have come up with a model that can at least tell you what tournaments like this one would look like in similar situations.

\newpage

\section{Overview}
Describe the problem, your model, results and how your model performed.

\newpage

\section{Introduction}
% ``Rephrase the problem
% Problems are open-ended and there are many ways to interpret and address them. Explain how you approached the problem."

People love sports because they are a showcase of hard work and talent. People also love sports because they are unpredictable. How do we quantify the effect of one versus the other in a basketball game? On the one hand, we often see this framed as a strict dichotomy between luck and skill: sports outcomes depend in some part on one and in some part on the other. However, careful examination reveals that it is rather unclear where to draw the line between luck and skill. For example, if a team happens to study exactly the right film the night before a big game and is able to perfectly foil the other team's plans, is that luck or is it skill? On the one hand, it certainly takes skill to study your opponent and use this information to your advantage. On the other hand, they could have easily picked the wrong film to study! Instead, we would like to approach this problem using a dichotomy between certainty and uncertainty.  This also addresses the issue of a lack of data; there are many aspects of ``skill'' (such as individual player records) which require more data than we have been able to gather so far.

Our goal here is twofold: first develop a model which predicts the outcome of a basketball game, and second quantify the effect of luck/uncertainty in a basketball game. Since we want to incorporate the effect of uncertainty, our prediction model will contain a stochastic element which we will have to derive. To train our model, we will use data from the regular season games leading up to the 2024 March Madness Tournament, and then test our model's ability to predict the outcome of the tournament. However, we will also discuss additional markers of uncertainty in our data to better understand the effect of uncertainty on a basketball game in a more general sense. 


\section{Methods}
% ``Explain your model
% Clearly state and justify ALL assumptions your model uses
% Motivate your model. Why did you choose your approach?
% Clearly describe your model
% Clearly define all variables
% Include tables and figures to make it easier to understand
% Analyze your model
% What are the strengths and weaknesses of your model?
% How could you test your model? How stable are the results to noise? 
% If you had more time, how would you expand/improve your model''?

Our model will be a modified version of the classic Elo ranking in chess, which we will use along with data from throughout the regular season to assign each team a ranking. (From here on, when we say ``Elo ranking'' we will mean our modified ranking, and will say ``classic Elo ranking'' if we mean the original.) Given a pair of teams, these rankings will be used to give us probabilities of an upset (that is, the probability that the team with the lower Elo rating wins). 


\subsection{Our ``Elo'' Ranking}
Traditionally (see~\cite{mediumRatingSystem}), the Elo ranking system first calculates an expected score for the game as a function of the difference in the two teams' Elo ratings. Here, $E$ is the ``main'' team while $E_{opp}$ is the opposing team:

\begin{equation}\label{eq:E}
E = \frac{1}{1+10^{(\gamma_{opp}-\gamma_n)/c}}
\end{equation}

where $\gamma_n$ is the main team's pre-game Elo score (after game $n$), $\gamma_{opp}$ is the opposing team's pre-game Elo score, and $c, K > 0$ are constants. The main team's Elo rating is updated after their $n+1^{\text{th}}$ game based on the following formula:

\begin{equation}
\gamma_{n+1} = \gamma_n + K*(O - E)
\end{equation}


Where $O$ is 1 if the team wins and 0 if the team loses. This is essentially performing a stochastic gradient descent (taking a step after each game) for a logistic regression model, where the Elo ratings are the weights of the model~\cite{stmorseStatisticalLearning}.

However, this score update does not take into account the score differential of the outcome of the game. Thus we propose a modified Elo system which uses the score outcomes to update a team's Elo score in one of the two following ways: 

\begin{equation}
\label{exponentialUpdate}
\gamma_{n+1} = \gamma_n + K^{\frac{S_{\text{win}}}{S_{\text{lose}}}}(O - E)
\end{equation}


\begin{equation}
\label{multiplyUpdate}
\gamma_{n+1} = \gamma_n + K*(O - E)*\frac{S_{\text{win}}}{S_{\text{lose}}}
\end{equation}


where $S_{\text{win}}$, $S_{\text{lose}}$ are the scores of the winner/loser and the other variables are defined as above. We will calculate $E$ in the same way setting $c$ to the canonical 400 for simplicity, and choose $K$ based on considerations described in the next section near a canonical value  of $K\approx 32$~\cite{mediumRatingSystem}.

We considered several possibilities for incorporating the score differential. We considered defining the score differential as the absolute value of the difference of the scores. However, this would weight a 20--10 loss as the same as a 90--80 loss, which seems much less accurate to how basketball fans see the game. Thus we would like to instead use the ratio $\frac{S_{\text{win}}}{S_{\text{lose}}}$. The question now becomes: how much do we want to allow this ratio to effect the score update? In \autoref{exponentialUpdate}, we allow it to have a much more significant effect than it does in \autoref{multiplyUpdate}. Ultimately, we decided to use both; we will discuss this further in the following section.

\subsection{Elo Calculation Algorithm}
Our goal is to have an Elo ranking for every team that made it to the 2024 March Madness Tournament, using the results of the games in the regular season. However, many of these regular season games were not played against the teams that made it to the tournament. Thus, we also needed to assign Elo ratings to teams that did not play in the regular season. Note there aren't many games for these non-tournament teams; we will discuss this issue below. 

We start all teams at the same Elo rating. This is typically a high positive value (to prevent giving a discouraging negative rating to low ranking chess players), but since the model just uses the difference in Elo ratings it doesn't matter what the initial value is. We chose to set it at 0 for simplicity. We then sort the games by date, and use one of the Elo update formulas to update the Elo ratings for every game. There are four cases to consider:

\begin{enumerate}
    \item The update for a tournament team after beating a non-tournament team. In this case, we use \autoref{multiplyUpdate}, as we don't want this game (or the score differential) to have a huge effect --- the tournament team should have won anyhow.
    \item The update for a non-tournament team playing any other team. In this case, we use \autoref{multiplyUpdate}; there aren't very many games for these teams, so we don't want their Elo rating to change too much from zero just based on a few games. Of course, we still want a tournament team's score to go down significantly if they lose to this team; see next point.
    \item The update for a tournament team after losing to a non-tournament team. In this case, we use \autoref{exponentialUpdate}, as we want the score differential to have a large effect. This tournament team is likely to do much worse in the tournament if it lost in a blowout to a team that didn't even make it to the tournament. As mentioned above, these non-tournament teams will have average Elo ratings, so we want to emphasize the fact that this really is a team the tournament team should have beat.
    \item The update for a tournament team after playing another a tournament team. In this case, we use \autoref{exponentialUpdate} to give the score differential a large effect; blowouts in this 
\end{enumerate}

After all of the games have created updates, we now have the Elo rating for every team that made it to the tournament. At this point, we will throw out the other teams that didn't make it to the tournament, as their Elo ratings are less meaningful due to the small numbers of games.

\subsection{Verifying Elo Rankings}\label{verify}
We would like to verify the accuracy of our Elo system. To do this, we go back to the samples from the regular season games to compare our model's predictions of the probability of an upset to the actual observed probability of an upset for a given difference in Elo rankings. Let $E_u(\Delta_{\text{Elo}})$ be the former probability and $\hat{p}_u(\Delta_{\text{Elo}})$ the latter, where $\Delta_{\text{Elo}}$ is the difference in (final) Elo ratings between the two teams. If our Elo ratings are accurate, we should see these are close to one another. 

First, we find the minimum and maximum differences in (final) Elo rankings for all of the games played. We then divvy this up into equal parts, as small as we can but with each partition large enough to contain a significant portion of the games. See JAKE INSERT FIG for a histogram of how many games fall into the partitions we chose. For each partition, we then calculate $E_u(\Delta_{\text{Elo}})$ via \autoref{eq:E} using the middle $\Delta_{\text{Elo}}$ for that partition. Let $N_i$ be the number of games played in the regular season. To calculate $\hat{p}_u(\Delta_{\text{Elo}})$, we simply take the number of upsets (lower Elo rating team wins) divided by $N_i$. This is the maximum likelihood estimate (MLE) for the probability of an upset in the given partition~\cite{statproofbookMaximumLikelihood}.

To quantify the accuracy of our Elo system, we first plot the Elo predicted upset probability $E_u(\Delta_{\text{Elo}})$ against the observed probability MLE $\hat{p}_u(\Delta_{\text{Elo}})$ for each value of $\Delta_{\text{Elo}}$. We then plot the $y=x$ line and calculate the MSE of these points with respect to this diagonal line. That is, we calculate the straight line distance from each point in 2D space to the $y=x$ line~\cite{enwiki:1235411332}:

\[
D_i = \frac{|E_u(\Delta_{\text{Elo, i}}) + \hat{p}_u(\Delta_{\text{Elo, i}})|}{{[E_u(\Delta_{\text{Elo, i}})]}^2+{[\hat{p}_u(\Delta_{\text{Elo, i}})]}^2}
\]

and average the squares of these distances:

\[
MSE = \frac{1}{M}\sum_{i=1}^M {(D_i)}^2
\]

where $M$ is the number of partitions. We then repeat this process for a range of $K$ values ($K \in {10, 15, 20, 25, 30, 35, 40}$), and choose the $K$ that minimizes the MSE.\@ (See INSERT REF TO PLOT.) Of course, this process increases the risk of overfitting to the regular season data. However, if we incorporate the tournament games, then we are not providing an unbiased assessment of our model's performance on the tournament data. See \autoref{nextSteps} for further discussion of overfitting.

\subsection{Bootstrapping Baseline Uncertainty}
As alluded to in the introduction, there is one partition we are particularly interested in: the one with the highest score differential. We will bootstrap many sample means to construct an approximate 95\% percentile bootstrap confidence interval (CI) for the probability of an upset in the highest score differential partition~\cite{uchicagoPercentileBootstrap}\cite{hesterberg_bootstrap_2015}. This will give an approximate 95\% CI for the ``baseline uncertainty'' of a basketball game in the March Madness tournament, provided we make the following assumptions:
\begin{enumerate}
    \item The Elo ratings are accurate.
    \item This maximum Elo rating difference is close to the true maximum Elo rating difference that would happen in a tournament. 
    \item This sample of high differential games is representative of all high differential games.
    \item The sample distribution of the sample mean is symmetric (this allows us to say we can cut off both 2.5\% tails of the distribution for our confidence interval).
    \item The bootstrapped sample distribution of the sample mean is representative of the true distribution of the sample mean.
\end{enumerate}

If are comfortable with these assumptions, we would then have an estimate range for the minimum amount of uncertainty that exists a tournament game, regardless of skill differential. With more time, we would have liked to explore the validity of these assumptions in more detail.
% 3. POSSIBLY Use the CI for p-hat to adjust Elo ratings and/or accept/reject a hypothesis about the actual p-hat.



\subsection{Weaknesses}
An obvious weakness of our model is our lack of data for the other teams in the league. Ideally, we would like to have accurate Elo ratings for these teams so that we can get accurate Elo updates for the tournament teams. We have tried to address this by changing the update formula for non-tournament teams, but ultimately we would prefer to just have more data for those teams. Another weakness is the previously mentioned risk of overfitting to the regular season data due to the selection of $K$. 

\subsection{Rejected Approaches}
We considered only use the games between tournament teams to caluclate the MLE of each partition upset probability. However, this would have seriously limited our already small dataset, so we decided against this. We also considered bootstrapping for the sample mean. However, this wouldn't do anything different as this bootstrapped average should just converge to the original sample mean. 

\section{Results}
% What does your model say about the question you have been given?
% In particular, you may consider whether
% incorporating the margin of victory in each game can change your model predictions of the contributions of
% ability and chance to the sports outcomes. You may also assess how accurately your model could predict the
% outcome of the 2024 March Madness Tournament.


\subsection{Verifying Elo Rankings}
As discussed in \autoref{verify}, we plot the Elo predicted upset probability $E_u(\Delta_{\text{Elo}})$ (based on the final regular season Elo ratings) against the observed bootstrapped probability $\hat{p}_u(\Delta_{\text{Elo}})$, as well as the $y=x$ line. If our Elo system has perfectly captured the expected upset probability, then these points should be close to the $y=x$ line. To measure the inaccuracy, we calculate the mean squared error. Plot Y shows the effect of $K$ on the MSE (as discussed in \autoref{verify}) while plot X uses the value of $K$ that minimized the MSE.\@ JAKE GENERATE PLOTS???

\subsection{Quantifying Uncertainty}
To first order, we now also have two (hopefully similar) ways of quantifying the measure of uncertainty in a basketball game: the expected probability of an upset based on Elo rankings (as a function of the difference in Elo rankings), and the MLE for the probability of an upset (as a function of the difference in Elo rankings). Using the expected probability as a measure of the uncertainty assumes that the formula used to calculate the expected probabilities given Elo rating is a good model for the probability of an upset. Using the bootstrapped probability as a measure of uncertainty assumes that we have enough data to be able to approximate the true probability of an upset for a given class of difference in Elo rankings. Both of these measures assume that our Elo rankings --- or at least their differences --- accurately capture the differences in skill level between teams (which is no small assumption). We will make this aassumption for the remainder of this paper; while our Elo system might not be completely accurate, it could be improved with more time and the incorporation of more data (see \autoref{nextSteps}).

As far as our basic underlying uncertainty --- that is, the uncertainty for a game with a maximum differential --- we found the confidence interval to be between BLANK and BLANK.\@ See (INSERT PLOT JAKE) for the bootstrap estimated distribution of the sample mean.


% Actually, I don't think we need to say either of these kinds of things. 
% We would like to somehow combine these two measures of uncertainty for our predictions. One option here is to simply average the two upset probabilities. Another option is ROHAN???


% We would also like to consider a ``higher order'' kind of uncertainty by comparing the bootstrapped probability of an upset to the expected probability of an upset. For example in cases where these probabilities differ significantly, we might consider increasing the chance of an upset??? ROHAN?\@

\subsection{Predicting March Madness 2024}
We would like to see how well our model could have predicted the results of the March Madness 2024 Tournament. To this end, we ran a large number of simulations using our Elo predictions by defaulting to the winner with the higher Elo but then switching our choice using the (Elo prediction, assuming we find its close enough to the MLEs) and ``roll a dice'' (with computer generated pseudo-random numbers) to determine whether an upset occurs. We then recorded what percentage of times we correctly predicted the outcome of the tournament. Since this was a very low value, we also recorded the average percentage of games for which we correctly predicted the outcome. JAKE?\@ Plots?


Of course, due to the uncertainty everyone acknowleges exists, this percentage would be low no matter what. After all, no one has ever correctly guessed the result of the tournament~\cite{cbsnewsAnyoneEver}. However, we note that our model (on average) performed better than just picking the winner as the one with the better seed. Using this strategy would have yielded correct predictions on (JAKE?)\% of the games.



\section{Next Steps}\label{nextSteps}
There are several things we would have liked to check if we had had more time. For one, it would be interesting to explore the effect of overfitting by the choice of $K$. That is, does the value of $K$ which minimized the MSE for the bootstrapping verification also maximize the likelihood of predicting the actual outcome of the tournament? If not, then we may want to look into a different way of picking $K$ (perhaps one which incorporates past data) as this choice seems to have lead to overfitting on the regular season games. We also would have liked to explore incorporating the score differential in different ways (such as $\gamma_{n+1} = \gamma_n + K^{\frac{S_{\text{win}}}{S_{\text{lose}}}}(O - E)$ as previously mentioned).

We would have liked to explore coming up with a prior distribution for the true ${p}_u(\Delta_{\text{Elo}})$ using our Elo expected ratings and then updating this prior via Bayesian inference using the observed upset game data. Moreover, we would have checked if all of the Elo expected probabilities were within approximate bootstrapped 95\% confidence intervals for $\hat{p}_u(\Delta_{\text{Elo}})$. This would have been a good sign that our Elo system is accurate. With more time, we also would have done more to explore the validity of the assumptions required for the approximate bootstrapped percentile confidence intervals to hold.

Another thing we would have liked to have done is to see how our MSE estimates would change if we added noise to the data; for example, we could have multiplied each score in the game by some rangomd number between 0.8 and 1.2 and seen if we would still have obtained a similar ideal value of $K$ and the similar MSE for this value. If so, this would indicate our Elo system is robust, and that our $K$ value is not overfit to the regular season data.

\subsection{Additional Data}
There are several things we would like to do if we had the time and resources to acquire more data.
\begin{itemize}
    \item We would have liked to incorporate the homefield advantage in the expected score calculated from the Elo system. If we had had this data, we would have added some number $A$ from the expected score of the home team and subtracted it from the expected score of the away team. For example, if we found that home teams on average win 53\% of games, then we would add/subtract $A=0.03$. This way, the expected score of two teams with the same Elo score would be $0.5 \pm 0.03$. 
    \item To pick the $K$ that minimizes the MSE for our Elo system compared to the bootstrapped estimate, we could use data from past seasons. We could also use the data from past tournaments, since those data may be more accurate for predicting tournament games (e.g.\ higher levels of variability due to high pressure scenario).
    \item As previously mentioned, it would have been ideal to have an accurate Elo rating for the other teams in the league, which would require regular season data for those teams.
\end{itemize}

\bibliographystyle{plain}
\bibliography{bibliography}

\newpage

\input{AI Use Report.tex}

\end{document}




% CANT DO THIS WITHOUT KNOWING TRUE P-HAT. could do this to see how often it covers E_0 but who cares? 2. Bootstrap CI's to estiate coverage probability (what percent of the bootstrapped CI's covered the )
% Note that the probability of an upset obtained via bootstrapping $\hat{p}_u(\Delta_{\text{Elo}})$ could be anything between 0 and 1, while the expected outcome probability from Elo rankings is necessarily between 0 and 1/2. However, we expect that $\hat{p}_u(\Delta_{\text{Elo}})$ should be roughly less than 0 to 1/2, for the same reasons that we hope our Elo expected outcomes $E_u(\Delta_{\text{Elo}})$ are similar to $\hat{p}_u(\Delta_{\text{Elo}})$ for each value of $(\Delta_{\text{Elo}})$. 
