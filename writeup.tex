\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage[margin=1in]{geometry}

\title{Luck vs Skill in College Basketball}
\author{Seth Peacock, Jake Browning, Rohan Mawalkar}
\date{November 17 2024}

\begin{document}

\maketitle

\newpage

\section{Letter to the Editor}
% In addition to this report, write a one-page letter to the newspaper chief editor, explaining the main results of the report and suggesting findings that can be communicated with the basketball fans reading the newspaper.

People love sports because they are a showcase of hard work and talent. People also love sports because they are unpredictable. How do we quantify the effect of one versus the other in a basketball game? On the one hand, we often see this framed as a strict dichotomy between luck and skill: sports outcomes depend in some part on one and in some part on the other. However, careful examination reveals that it is rather unclear where to draw the line between luck and skill. For example, if a team happens to study exactly the right film the night before a big game and is able to perfectly foil the other team's plans, is that luck or is it skill? On the one hand, it certaintly takes skill to study your opponent and use this information to your advantage. On the other hand, they could have easily picked the wrong film to study! Instead, it is better to approach this problem using a dichotomy between certainty and uncertainty. Certainty is how sure we can be about the outcome of a game using past data (ranking, players' stats, etc). But it is almost surely the case that --- even if you utilized all the data sports analysts have collected so far --- there's always some chance of an upset.

Year after year, people fill out brackets to try to predict the March Madness tournament. Year after year, not a single person is able to correctly guess the whole thing~(\cite{cbsnewsAnyoneEver}). This is a clear indication that the tournament is just too uncertain to expect to be able to predict. We're not claiming anything deeply metaphysical here. It could be true that if you knew the precise location of all the relevant atoms in all the stadiums and in all the players and in everything they interact with then you could predict the outcome of the NCAA tournament perfectly. The question we are interested in is different: \textit{given the data we have}, what is the minimum amount of uncertainty that still exists in a basketball game? That is, what is the minimum chance of an upset, no matter how different in observed skill two teams are?

The first step in our approach is to assign ratings to each team. We do this via a modified version of the classic Elo rating system in chess. Each team starts at the same Elo rating. We then walk through the regular season game by game, and after each game we update the Elo rating of each team based on three factors: whether the team won or lost, by how many points, and the Elo rating (how ``good'') the other team was. If you win, your rating goes up, and if you lose it goes down. If you win/lose in a blowout, it would go up/down a lot. If you beat a team that's much better than you, you go up by a lot, while if you beat a team that's much worse than you (or lose to a team much better than you), you don't change much. Our modifiications to the original rating system include the incorporation of score differential and the possibility for much larger changes in a single game (to allow for significant changes to take effect over the course of a single season.)

After stepping through all games in the season, we hope that our ratings are reasonably accurate. The beauty of the Elo ratings is that they inherently provide the expected probability of an upset given a skill level difference. Therefore to verify our end of season ratings, we can look at all of the games with a given skill level difference, and compare the predicted probability of an upset to the observed percentage of those games which had an upset. If these are close, this is a good indication that our ratings are accurate.

Now that we have our ratings, there are two things we can do with them. First, we can go ahead and simulate the tournament. For each game in the bracket, we calculate the probability of an upset based on the difference in their Elo ratings. We then ``roll a die'' to find out if there was an upset, based on the previously calculated probabilty. For example, say a 6 seed is playing a 11 seed in the first round. Based on the Elo rating differential (NOT based on the seeds, which any basketball fan will tell you are poor estimations of actual skill), we caluclate that the probability of an upset is 1/6. We then roll a die --- if it lands on 6, we say there is an upset and the 11 seed wins. If it lands on anything else, we say the 6 seed wins.

We are not claiming that running this simulation will predict the outcome of the tournament. After all, we've baked in the uncertainty when we rolled the die! The best we can do is come up with a model that can at least tell you what tournaments like this one could look like, which perhaps isn't of much interest. However, our original question was about the minimum amount of uncertainty in a basketball game. This we can now start to answer. First, we find the biggest Elo score differential that we found to exist between two teams. Next, we look at all of the past games between two teams that were this maximum distance apart in skill, and estimate the probability of an upset in this category. Ideally, \textit{this percentage is then our desired minimum uncertainty}. For us, we found this number to be between JAKE?\@. 

There's just one problem: we were looking for the minimum amount of uncertainty \textit{when all of the available data was used.} Whatever constitues \enquote{all of the available data} is going to be a massive store created over the past decades which we have neither the time, resources, or access to process. We only used scores for the 2024 regular season games. Ideally, we would also incorporate scores for past seasons (especially past tournament games), assign Elo rankings to individual players, consider injuries, and so on. All of this is certainly doable, but is beyond the scope of our project. The more data we incorporate, the less this expected minimum uncertainty should be. However, unless we get to the point where we're tracking the precies positions of all of the atoms, this minimum upset probability will always be nonzero. Since we live in a world where any mention of the bizzarities of quantum mechanics scintillates the masses, we'll make the comparison before someone else does: perhaps it would be apt (if not scientifically founded) to think of the ideal minimum upset probability as the manifestation of a \enquote{basketball uncertainty principle.}

\newpage

\section{Overview}
Describe the problem, your model, results and how your model performed.

\newpage

\section{Introduction}
% ``Rephrase the problem
% Problems are open-ended and there are many ways to interpret and address them. Explain how you approached the problem."
In this paper, we explore the relationship between skill and luck in college basketball. Our goal here is twofold: first develop a model which predicts the outcome of a basketball game, and second quantify the effect of luck/uncertainty in a basketball game. Since we want to incorporate the effect of uncertainty, our prediction model will contain a stochastic element which we will have to derive. To train our model, we will use data from the regular season games leading up to the 2024 March Madness Tournament, and then test our model's ability to predict the outcome of the tournament. Of course, we do not expect to be able to actually predict the outcome of the tournament. We won't go as far as to say that it is impossible to predict the outcome; it could be true that if you knew the precise location of all the relevant atoms in all the stadiums and in all the players and in everything they interact with then you could predict the outcome of the NCAA tournament perfectly. The question we are interested in is different: \textit{given the data we have}, what is the minimum amount of uncertainty that still exists in a basketball game? That is, what is the minimum chance of an upset, no matter how different in observed skill two teams are? To address the second question, we will quantify the effect of uncertainty in a basketball game by estimating the probability of an upset in the most extreme case of skill differential. 

\section{Methods}
% ``Explain your model
% Clearly state and justify ALL assumptions your model uses
% Motivate your model. Why did you choose your approach?
% Clearly describe your model
% Clearly define all variables
% Include tables and figures to make it easier to understand
% Analyze your model
% What are the strengths and weaknesses of your model?
% How could you test your model? How stable are the results to noise? 
% If you had more time, how would you expand/improve your model''?

Our model will be a modified version of the classic Elo ranking in chess, which we will use along with data from throughout the regular season to assign each team a ranking. (From here on, when we say ``Elo ranking'' we will mean our modified ranking, and will say ``classic Elo ranking'' if we mean the original.) Given a pair of teams, these rankings will be used to give us probabilities of an upset (that is, the probability that the team with the lower Elo rating wins). 


\subsection{Our ``Elo'' Ranking}
Traditionally, (see~\cite{mediumRatingSystem}), the Elo ranking system first calculates an expected score for the game as a function of the difference in the two teams' Elo ratings. Here, $E$ is the probability that the ``main'' team wins, while $E_{opp}$ is the probability that the opposing team wins:

\begin{equation}\label{eq:E}
E = \frac{1}{1+10^{(\gamma_{opp}-\gamma_n)/c}}
\end{equation}
\begin{equation}\label{eq:E}
E_{opp} = \frac{1}{1+10^{(\gamma_n-\gamma_{opp})/c}}
\end{equation}

where $\gamma_n$ is the main team's pre-game Elo score (after game $n$), $\gamma_{opp}$ is the opposing team's pre-game Elo score, and $c, K > 0$ are constants. Note that $E + E_{opp} = 1$. The main team's Elo rating is updated after their $n+1^{\text{th}}$ game based on the following formula:

\begin{equation}
\gamma_{n+1} = \gamma_n + K*(O - E)
\end{equation}


Where $O$ is 1 if the team wins and 0 if the team loses. Since E is a probability,  $E \in [0, 1]$. This means that a team winning or losing uniquely determines whether they gain or lose points on their Elo rating. This is essentially performing a stochastic gradient descent (taking a step after each game) for a logistic regression model, where the Elo ratings are the weights of the model~\cite{stmorseStatisticalLearning}.

However, this score update does not take into account the score differential of the outcome of the game, given its origin in chess, a scoreless game. Thus we propose a modified Elo system which uses the score outcomes to update a team's Elo score in one of the two following ways: 

\begin{equation}
\label{exponentialUpdate}
\gamma_{n+1} = \gamma_n + K^{\frac{S_{\text{win}}}{S_{\text{lose}}}}(O - E)
\end{equation}


\begin{equation}
\label{multiplyUpdate}
\gamma_{n+1} = \gamma_n + K*(O - E)*\frac{S_{\text{win}}}{S_{\text{lose}}}
\end{equation}


where $S_{\text{win}}$, $S_{\text{lose}}$ are the scores of the winner/loser and the other variables are defined as above. These changes mean that a team's Elo rating will change by a larger amount when the score differential is larger, and vice-versa. We will calculate $E$ in the same way as setting $c$ to the canonical 400 for simplicity, and choose $K$ based on considerations described in the next section near a canonical value  of $K\approx 32$~\cite{mediumRatingSystem}.

We considered several possibilities for incorporating the score differential. We considered defining the score differential as the absolute value of the difference of the scores. However, this would weight a 20--10 loss as the same as a 90--80 loss, which seems much less accurate to how basketball fans see the game. Thus, we would like to instead use the ratio $\frac{S_{\text{win}}}{S_{\text{lose}}}$. The question now becomes: how much do we want to allow this ratio to effect the score update? In \autoref{exponentialUpdate}, we allow it to have a much more significant effect than it does in \autoref{multiplyUpdate}. Ultimately, we decided to use both; we will discuss this further in the following section.

\subsection{Elo Calculation Algorithm}
Our goal is to have an Elo ranking for every team that made it to the 2024 March Madness Tournament, using the results of the games in the regular season. However, many of these regular season games were not played against the teams that made it to the tournament. Thus, we also needed to assign Elo ratings to teams that did not play in the regular season. Note that there aren't many games for these non-tournament teams; we will discuss this issue below. 

We start all teams at the same Elo rating. This is typically a high positive value (to prevent giving a discouraging negative rating to low ranking chess players), but since the model solely uses the difference in Elo ratings, it doesn't matter what the initial value is. We chose to set it at 0 for simplicity. We then sort the games by date, and use one of the Elo update formulas to update the Elo ratings for every game. There are four cases to consider:

\begin{enumerate}
    \item The update for a tournament team after beating a non-tournament team. In this case, we use \autoref{multiplyUpdate}, as we don't want this game (or the score differential) to have a huge effect --- the tournament team should have won anyhow.
    \item The update for a non-tournament team playing any other team. In this case, we use \autoref{multiplyUpdate}; there aren't very many games for these teams, so we don't want their Elo rating to change too much from zero just based on a few games. Of course, we still want a tournament team's score to go down significantly if they lose to this team; see next point.
    \item The update for a tournament team after losing to a non-tournament team. In this case, we use \autoref{exponentialUpdate}, as we want the score differential to have a large effect. This tournament team is likely to do much worse in the tournament if it lost in a blowout to a team that didn't even make it to the tournament. As mentioned above, these non-tournament teams will have average Elo ratings, so we want to emphasize the fact that this really is a team the tournament team should have beat.
    \item The update for a tournament team after playing another a tournament team. In this case, we use \autoref{exponentialUpdate} to give the score differential a large effect; blowouts in this case should have a large effect on Elo ratings. 
\end{enumerate}

After all of the games have created updates, we now have the Elo rating for every team that made it to the tournament. At this point, we will throw out the other teams that didn't make it to the tournament, as their Elo ratings are less meaningful due to the small numbers of games.

\subsection{Verifying Elo Rankings}\label{verify}
We would like to verify the accuracy of our Elo system. To do this, we go back to the samples from the regular season games to compare our model's predictions of the probability of an upset to the actual observed probability of an upset, for a given difference in Elo rankings. Let $E_u(\Delta_{Elo})$ and $\hat{p}_u(\Delta_{Elo})$ be functions of $\Delta_{Elo}$, the difference in (final) Elo ratings between two teams. Respectively, these represent the predicted and observed probabilities of an upset. If our Elo ratings are accurate, we should see these values are close to one another.

First, we find the minimum and maximum differences in (final) Elo rankings for all of the games played. We then divvy this up into equal parts, as small as we can but with each partition large enough to contain a significant portion of the games. See JAKE INSERT FIG for a histogram of how many games fall into the partitions we chose. For each partition, we then calculate $E_u(\Delta_{\text{Elo}})$ via \autoref{eq:E} using the middle $\Delta_{\text{Elo}}$ for that partition. Let $N_i$ be the number of games played in the regular season. To calculate $\hat{p}_u(\Delta_{\text{Elo}})$, we simply take the number of upsets (lower Elo rating team wins) divided by $N_i$. This is the maximum likelihood estimate (MLE) for the probability of an upset in the given partition~\cite{statproofbookMaximumLikelihood}.

To quantify the accuracy of our Elo system, we first plot the Elo predicted upset probability $E_u(\Delta_{\text{Elo}})$ against the observed probability MLE $\hat{p}_u(\Delta_{\text{Elo}})$ for each value of $\Delta_{\text{Elo}}$. We then plot the $y=x$ line and calculate the MSE of these points with respect to this diagonal line. That is, we calculate the straight line distance from each point in 2D space to the $y=x$ line~\cite{enwiki:1235411332}:

\[
D_i = \frac{|E_u(\Delta_{\text{Elo, i}}) + \hat{p}_u(\Delta_{\text{Elo, i}})|}{{[E_u(\Delta_{\text{Elo, i}})]}^2+{[\hat{p}_u(\Delta_{\text{Elo, i}})]}^2}
\]

and average the squares of these distances:

\[
MSE = \frac{1}{M}\sum_{i=1}^M {(D_i)}^2
\]

where $M$ is the number of partitions. We then repeat this process for a range of $K$ values ($K \in {10, 15, 20, 25, 30, 35, 40}$), and choose the $K$ that minimizes the MSE.\@ (See INSERT REF TO PLOT.) Of course, this process increases the risk of overfitting to the regular season data. However, if we incorporate the tournament games, then we are not providing an unbiased assessment of our model's performance on the tournament data. See \autoref{nextSteps} for further discussion of overfitting.

\subsection{Bootstrapping the Basketball Uncertainty Principle}
As alluded to in the introduction, there is one partition we are particularly interested in: the one with the highest score differential. We will bootstrap many sample means to construct an approximate 95\% percentile bootstrap confidence interval (CI) for the probability of an upset in the highest score differential partition~\cite{uchicagoPercentileBootstrap}\cite{hesterberg_bootstrap_2015}. This will give an approximate 95\% CI for the ``baseline uncertainty'' of a basketball game in the March Madness tournament, provided we make the following assumptions:
\begin{enumerate}
    \item The Elo rating system is able to accurately capture all of the certainty that is currently able to be gleaned from pre-tournament data.
    \item The ELo rating system \textit{has} accurately captured all of the certainty that is currently able to be gleaned from pre-tournament data.
    \item This maximum Elo rating difference is close to the true maximum Elo rating difference that would happen in a tournament. 
    \item This sample of high differential games is representative of all high differential games.
    \item The sample distribution of the sample mean is symmetric (this allows us to say we can cut off both 2.5\% tails of the distribution for our confidence interval).
    \item The bootstrapped sample distribution of the sample mean is representative of the true distribution of the sample mean.
\end{enumerate}

If are comfortable with these assumptions, we would then have an estimate range for the minimum amount of uncertainty that exists a tournament game, regardless of skill differential. With more time, we would have liked to explore the validity of these assumptions in more detail. We discuss the most glaringly obviously unsatisfied one in the next section.


% 3. POSSIBLY Use the CI for p-hat to adjust Elo ratings and/or accept/reject a hypothesis about the actual p-hat.



\subsection{Weaknesses}
An obvious weakness of our model is our lack of data for the other teams in the league. Ideally, we would like to have accurate Elo ratings for these teams so that we can get accurate Elo updates for the tournament teams. We have tried to address this by changing the update formula for non-tournament teams, but ultimately we would prefer to just have more data for those teams. But this is just a small part of the problem; clearly, there is much much more pre-tournament data out there that could be useful, and assumption \#2 above is false. All we are using in our model is the win/loss margins of the regular season games. For this reason, we do not claim that our observed derived minimum amount of uncertainty is an accurate estimate of the true uncertainty when all currently available data is utilized. The more information used, the more opportunities there are to turn uncertainty into certainty. Our claim is that this is an estimate for the uncertainty \textit{given the data we used}. We discuss different types of data we could incorporate in \autoref{additionalData}. Another weakness is the previously mentioned risk of overfitting to the regular season data due to the selection of $K$. We explore possible solutions in \autoref{nextSteps}.

\subsection{Rejected Approaches}
We considered only use the games between tournament teams to caluclate the MLE of each partition upset probability. However, this would have seriously limited our already small dataset, so we decided against this. 

% We also considered bootstrapping for the sample mean. However, this wouldn't do anything different as this bootstrapped average should just converge to the original sample mean. 

\section{Results}
% What does your model say about the question you have been given?
% In particular, you may consider whether
% incorporating the margin of victory in each game can change your model predictions of the contributions of
% ability and chance to the sports outcomes. You may also assess how accurately your model could predict the
% outcome of the 2024 March Madness Tournament.


\subsection{Verifying Elo Rankings}
As discussed in \autoref{verify}, we plot the Elo predicted upset probability $E_u(\Delta_{\text{Elo}})$ (based on the final regular season Elo ratings) against the observed bootstrapped probability $\hat{p}_u(\Delta_{\text{Elo}})$, as well as the $y=x$ line. If our Elo system has perfectly captured the expected upset probability, then these points should be close to the $y=x$ line. To measure the inaccuracy, we calculate the mean squared error. Plot Y shows the effect of $K$ on the MSE (as discussed in \autoref{verify}) while plot X uses the value of $K$ that minimized the MSE.\@ JAKE GENERATE PLOTS???

\subsection{Quantifying Uncertainty}
To first order, we now also have two (hopefully similar) ways of quantifying the measure of uncertainty in a basketball game: the expected probability of an upset based on Elo rankings (as a function of the difference in Elo rankings), and the MLE for the probability of an upset (as a function of the difference in Elo rankings). Using the expected probability as a measure of the uncertainty assumes that the formula used to calculate the expected probabilities given Elo rating is a good model for the probability of an upset. Using the bootstrapped probability as a measure of uncertainty assumes that we have enough data to be able to approximate the true probability of an upset for a given class of difference in Elo rankings. Both of these measures assume that our Elo rankings --- or at least their differences --- accurately capture the differences in skill level between teams (which is no small assumption). We will make this aassumption for the remainder of this paper; while our Elo system might not be completely accurate, it could be improved with more time and the incorporation of more data (see \autoref{nextSteps}).

As far as our ``basketball uncertainty principle'' --- that is, the uncertainty for a game between the best and the worst tournament team --- we found the confidence interval to be between BLANK and BLANK.\@ See (INSERT PLOT JAKE) for the bootstrap estimated distribution of the sample mean.
 

% Actually, I don't think we need to say either of these kinds of things: 
% We would like to somehow combine these two measures of uncertainty for our predictions. One option here is to simply average the two upset probabilities. Another option is ROHAN???


% We would also like to consider a ``higher order'' kind of uncertainty by comparing the bootstrapped probability of an upset to the expected probability of an upset. For example in cases where these probabilities differ significantly, we might consider increasing the chance of an upset??? ROHAN?\@

\subsection{Predicting March Madness 2024}
We would like to see how well our model could have predicted the results of the March Madness 2024 Tournament. To this end, we ran a large number of simulations using our Elo predictions by defaulting to the winner with the higher Elo but then switching our choice using the (Elo prediction, assuming we find its close enough to the MLEs) and ``roll a dice'' (with computer generated pseudo-random numbers) to determine whether an upset occurs. We then recorded what percentage of times we correctly predicted the outcome of the tournament. Since this was a very low value, we also recorded the average percentage of games for which we correctly predicted the outcome. JAKE?\@ Plots?


Of course, due to the uncertainty everyone acknowleges exists, this percentage would be low no matter what. After all, no one has ever correctly guessed the result of the tournament~\cite{cbsnewsAnyoneEver}. However, we note that our model (on average) performed better than just picking the winner as the one with the better seed. Using this strategy would have yielded correct predictions on 41/60 of the games (not counting the preliminaries or Final Four in which there were teams with the same seed playing against each other)~\cite{wikipedia2024NCAA}.

\section{Next Steps}\label{nextSteps}
\subsection{Overfitting}
There are several things we would have liked to check if we had had more time. For one, it would be interesting to explore the effect of overfitting by the choice of $K$. That is, does the value of $K$ which minimized the MSE for the bootstrapping verification also maximize the likelihood of predicting the actual outcome of the tournament? If not, then we may want to look into a different way of picking $K$ (perhaps one which incorporates past data) as this choice would seem to have lead to overfitting on the regular season games. Another thing we would have liked to have done is to see how our MSE estimates would change if we added noise to the data; for example, we could have multiplied each score in the game by some rangomd number between 0.8 and 1.2 and seen if we would still have obtained a similar ideal value of $K$ and the similar MSE for this value. If so, this would indicate our Elo system is robust, and that our $K$ value is not overfit to the regular season data.

\subsection{Reiterating Elo Scores}
Even without more data, we also could have likely improved the accuracy of our Elo rankings by reiterating through the season multiple times. That is, after we've reached the end of the season, we just start at the beginning and continue to update Elo rankings after each game. It would have been interesting to see if this process causes each team's Elo rankings to converge to particular values, and explore the effect of different $K$ on the relative difference between these. For example, could it be the case that despite $K$ causing the sizes of the Elo scores to change, they converge to values where the \textit{differences} between scores are approximately the same? This would indicate a robustness in the Elo system to different choices of $K$.

\subsection{Bayesian Estimation}
Second, we would have looked for a way to determine a prior distribution for the true ${p}_u(\Delta_{\text{Elo}})$ using our Elo expected ratings and then updating this prior via Bayesian inference using the observed upset game data to obtain a posterior distribution. We would have then found the maximum a posteriori (MAP) estimate (the parameter value that maximizes the posterior distribution~cite{probabilitycourseMaximumPosteriori}) for the probability of an upset in the highest score differential category.

\subsection{Confidence Intervals}
Third, we would have checked if all of the Elo expected probabilities were within approximate bootstrapped 95\% confidence intervals for $\hat{p}_u(\Delta_{\text{Elo}})$. This would have been a good sign that our Elo system is accurate. With more time, we also would have done more to explore the validity of the assumptions required for the approximate bootstrapped percentile confidence intervals to hold.

\subsection{Additional Data}\label{additionalData}
There are a few ideas we would have liked to implement if we had had more data on hand.
\begin{itemize}
    \item We would have liked to incorporate the homefield advantage in the expected score calculated from the Elo system. If we had had this data, we would have quantified the homefield advantage by finding the additional average win proportion $A$ of home teams vs that of away teams (say, over the past ten years). We would then add $A$ to the expected score of any home team in a game and subtract it from the expected score of the away team. For example, if we found that home teams on average win 53\% of games, then we would add/subtract $A=0.03$. This way, the expected score of two teams with the same Elo score would be $0.5 \pm 0.03$. 
    \item To pick the $K$ that minimizes the MSE for our Elo system compared to the observed MLE probability, we could use data from past seasons in the same way we did with the data we have now and find the $K$ that minimizes the sum of the MSEs from the different seasons. Even better, we could use the data from past tournaments, since those data may be more accurate for predicting tournament games (e.g.\ higher levels of variability due to high pressure scenario).
    \item As previously mentioned, it would have been ideal to have an accurate Elo rating for the other teams in the league, which would require regular season data for those teams.
    \item There are many ways we could improve our Elo system with individual player data. For example, one idea would be to give each of the starting members of every team an Elo score, in addition to the team itself (to quantify how well the team plays together). The expected scores would then be based on the differences in some combination of the Elo scores (perhaps a simple average of the players' scores added to the team's score). Updates would be done to the team's scores by the same method as above, while players' scores would change based on some combination of their offensive/defensive stats in the game.
\end{itemize}

\bibliographystyle{plain}
\bibliography{bibliography}

\newpage

\input{AI Use Report.tex}

\end{document}




% CANT DO THIS WITHOUT KNOWING TRUE P-HAT. could do this to see how often it covers E_0 but who cares? 2. Bootstrap CI's to estiate coverage probability (what percent of the bootstrapped CI's covered the )
% Note that the probability of an upset obtained via bootstrapping $\hat{p}_u(\Delta_{\text{Elo}})$ could be anything between 0 and 1, while the expected outcome probability from Elo rankings is necessarily between 0 and 1/2. However, we expect that $\hat{p}_u(\Delta_{\text{Elo}})$ should be roughly less than 0 to 1/2, for the same reasons that we hope our Elo expected outcomes $E_u(\Delta_{\text{Elo}})$ are similar to $\hat{p}_u(\Delta_{\text{Elo}})$ for each value of $(\Delta_{\text{Elo}})$. 
